\section{Question 4}

Let $X$ be the random variable for the number of people received the direct mail strategy and compelted screening.
Let $Y$ be the random variable for the number of people who received the education only outreach and completed screening.

\subsection{Part a)}

The notation $p_\X$ and $p_\Y$ represent the population proportion for $\X$ and $\Y$ respectively.

The null hypothesis is that both population proportions are equal: $\text{H}_0: p_\X = p_\Y = p$. The alternative hypothesis is therefore: $\text{H}_1 : p_\X > p_\Y$.

\subsection{Part b)}

$\X \sim \BinDist (n_\X, p_\X)$ where $n_\X = 1415$ so $\X \sim \BinDist (1415, p_\X)$.
It is (implicitely) assumed that samples $\X_i$ from $\X$ follow the distribution of $\X$ and are all independant, hence:
\[
\X_i \sim \BinDist(n_\X, p_\X)
\]
Since $n_\X p_\X = 505 \gg 5$ and $n_\X(1-p_\X)=910 \gg 5$, the conditions for the Central Limit Theorem (CLT) to be a good approximation are met, as well as a suitably large $n_\X$. Hence the CLT is reasonable for the research problem.
Therefore:
\[
\X_i \mathop \sim ^\text{approx} \NormalDist (n_\X p_\X, n_\X p_\X (1 - p_\X))
\]
\[
\bar{\X} = \hat{P}_\X \mathop \sim ^\text{approx} \NormalDist (p_\X, \frac{p_\X (1 - p_\X)}{n_\X})
\]

Under $\text{H}_0$:
\[
\hat{P}_\X \sim \NormalDist (p, \frac{p (1 - p)}{n_\X})
\]


$\Y \sim \BinDist (n_\Y, p_\Y)$ where $n_\Y = 1408$ so $\Y \sim \BinDist (1408, p_\Y)$.
It is (implicitely) assumed that samples $\Y_i$ from $\Y$ follow the distribution of $\Y$ and are all independant, hence:
\[
\Y_i \sim \BinDist(n_\Y, p_\Y)
\]
Since $n_\Y p_\Y = 264 \gg 5$ and $n_\Y(1-p_\Y)=1144 \gg 5$, the conditions for the Central Limit Theorem (CLT) to be a good approximation are met, as well as a suitably large $n_\Y$. Hence the CLT is reasonable for the research problem.
Therefore:
\[
\Y_i \mathop \sim ^\text{approx} \NormalDist (n_\Y p_\Y, n_\Y p_\Y (1 - p_\Y))
\]
\[
\bar{\Y} = \hat{P}_\Y \mathop \sim ^\text{approx} \NormalDist (p_\Y, \frac{p_\Y (1 - p_\Y)}{n_\Y})
\]

Under $\text{H}_0$:
\[
\hat{P}_\Y \sim \NormalDist (p, \frac{p (1 - p)}{n_\Y})
\]

It is additionally assumed that $\X$ and $\Y$ are independant from each other.

We can now give notation for the specific sample information we are given:
$\bar{x} = \hat{p}_\X = \frac{505}{1415} \approx 0.3568$ and $\bar{y} = \hat{p}_\Y = \frac{264}{1408} \approx 0.1875$

\subsection{Part c)}

\[
\hat{P}_\X - \hat{P}_\Y \sim \NormalDist (p_\X - p_\Y, \frac{p_\X(1-p_\X)}{n_\X} + \frac{p_\Y(1-p_\Y)}{n_\Y})
\]

Under $\text{H}_0$, or assuming $\text{H}_0$:

\[
\hat{P}_\X - \hat{P}_\Y \sim \NormalDist (0, \frac{p(1-p)}{n_\X} + \frac{p(1-p)}{n_\Y})
\]

To find a pivital variable that doesn't depend on the unknown $p$, a pooled unbiased estimator $\hat{P} = \frac{\X + \Y}{n_\X + n_\Y}$
will be used in place of $p = \hat{P}$. For our sample $\hat{P} = \frac{505 + 264}{1415 + 1408} \approx 2.724$. Rearranging gives:

\[
T = \frac{\hat{P}_\X - \hat{P}_\Y}{\hat{P}(1 - \hat{P})(\frac{1}{n_\X} + \frac{1}{n_\Y})} \sim \NormalDist(0, 1)
\]

The $\text{p-value}$ is therefore computable relative to our specific sample:
\[
\text{p-value} = \pr{\hat{P}_\X - \hat{p}_\X \geq \hat{P}_\Y - \hat{p}_\Y}
\]
In a slightly more useful arrangement:
\[
\text{p-value} = \pr{\hat{P}_\X - \hat{P}_\Y \geq \hat{p}_\X - \hat{p}_\Y}
\]
\[
\text{p-value} = \pr{
	\frac{\hat{P}_\X - \hat{P}_\Y}{\hat{P}(1 - \hat{P})(\frac{1}{n_\X} + \frac{1}{n_\Y})}
	\geq
	\frac{\hat{p}_\X - \hat{p}_\Y}{\hat{P}(1 - \hat{P})(\frac{1}{n_\X} + \frac{1}{n_\Y})}
}
\]

Note $\hat{p}_\X - \hat{p}_\Y \approx 0.16939$ and $\hat{P}(1 - \hat{P})(\frac{1}{n_\X} + \frac{1}{n_\Y}) \approx 0.01676$, hence:
\[
\text{p-value} = \pr{ \Z \geq \frac{0.16939}{0.01676}}
\]
\[
\text{p-value} = \pr{ \Z \geq 10.1079}
\]
\[
\text{p-value} = 1 - \pr{ \Z < 10.1079}
\]

Looking at the stats table, this is way off the charts! $\Phi(3.69) = 0.9999$, so
\[
\text{p-value} < 1 - 0.9999
\]
\[
\text{p-value} < 0.0001
\]
This is very strong evidence to reject the null hypothesis $\text{H}_0$.
Therefore we can conclude we have very strong evidence that direct-mail self-sampling kits have a higher screening population proportion than
education only outreach.

\subsection{Part d)}

A 97\% confidence interval means $\alpha = 3\% = 0.03$. The formula for a 2-sample binomial confidence interval is as follows:
\[
\hat{P}_\X - \hat{P}_\Y \pm z_{\frac{\alpha}{2}} \sqrt{\frac{\hat{P}_\X(1 - \hat{P}_\X)}{n_\X} + \frac{\hat{P}_\Y(1 - \hat{P}_\Y)}{n_\Y}}
\]
$z_{\frac{\alpha}{2}}$ is the the solution to:
\[
\pr{\Z > z_{\frac{\alpha}{2}}} = \frac{\alpha}{2} = 0.015
\]
\[
\pr{\Z < z_{\frac{\alpha}{2}}} = 0.985
\]
From the stats table, $z_{\frac{\alpha}{2}} = 2.17$. Plugging in our specific sample notation:

\[
\hat{p}_\X - \hat{p}_\Y \pm 2.17 \sqrt{\frac{\hat{p}_\X(1 - \hat{p}_\X)}{n_\X} + \frac{\hat{p}_\Y(1 - \hat{p}_\Y)}{n_\Y}}
\]

Evaluating yields the CI $(0.1337, 0.2051)$ for $p_\X - p_\Y$.
This confidence interval does not contain 0, therefore this is evidence that the proportions $p_\X$ and $p_\Y$ are not equal.

\subsection{Part e)}
\[
\X \sim \BinDist (n_\X, p_\X)
\]
\[
\Y \sim \BinDist (n_\Y, p_\Y)
\]

Under $\text{H}_0$ $p_\X = p_\Y = p$

\[
\Var(\X) = n_\X p (1 - p)
\]
\[
\Var(\Y) = n_\Y p (1 - p)
\]

\[
\hat{P}^w = w \frac{\X}{n_\X} + (1 - w)\frac{\Y}{n_\Y}
\]

To show $\Expected(\hat{P}^w) = p$ under $\text{H}_0$:

\begin{align*}
\Expected(\hat{P}^w) &= \Expected(w\frac{\X}{n_\X}) + \Expected((1-w)\frac{\Y}{n_\X}) \\
&= w \E(\frac{\X}{n_\X}) + (1-w)\E(\frac{\Y}{n_\Y}) \\
&= wp + (1-w)p \\
&= (w + 1 - w)p \\
&= p
\end{align*}

To show the variance, we must additionally assume that $\X$ and $\Y$ are independant:

\begin{align*}
\Var(\hat{P}^w) &= \Var(w \frac{\Y}{n_\X} + (1-w)\frac{\Y}{n_\Y}) \\
&= w^2 \Var(\frac{\X}{n_\X}) + (1-w)^2 \Var(\frac{\Y}{n_\Y}) \\
&= w^2 \frac{p(1-p)}{n_\X} + (1-w)^2 \frac{p(1-p)}{n_\Y} \\
&= p(1-p)\left( \frac{w^2}{n_\X} + \frac{(1-w)^2}{n_\Y} \right)
\end{align*}

\subsection{Part f)}
To find if a certain $w$ value minimuzes $\hat{P}^w$, I will take the derivative to find if it is a stationary point and the double deriviative
to confirm it is a minimum.

\[
\frac{\diff}{\diff w} \hat{P}^w = \frac{\diff}{\diff w} p(1-p)\left( \frac{w^2}{n_\X} + \frac{(1-w)^2}{n_\Y} \right)
\]

To solve this derivative sensibly, I will assume $0 = \frac{\diff p}{\diff w} = \frac{\diff n_\X}{\diff w} = \frac{\diff n_\Y}{\diff w}$,
as in, our choice of $w$ is independant of $p$, $n_\X$ and $n_\Y$.

\begin{align*}
\frac{\diff}{\diff w} \hat{P}^w &= p(1-p)\left[ \frac{1}{n_\X} \frac{\diff}{\diff w}(w^2) + \frac{1}{n_\Y} \frac{\diff}{\diff w}((1-w)^2) \right] \\
&= p(1-p) \left( \frac{2w}{n_\X} + \frac{-2(1-w)}{n_\Y} \right) \\
&= 2p(1-p) \left( \frac{w}{n_\X} + \frac{w}{n_\Y} - \frac{1}{n_\Y} \right) \\
\frac{\diff^2}{\diff w^2} \hat{P}^w &= 2p(1-p)\left( \frac{1}{n_\X} \frac{\diff}{\diff w}(w) + \frac{1}{n_\Y}\frac{\diff}{\diff w}(w) - \frac{1}{n_\Y} \right) \\
&= \frac{2p(1 - p)}{n_\X}
\end{align*}

Substituting $w = \frac{n_\X}{n_\Y + n_\Y}$:

\begin{align*}
\left. \frac{\diff}{\diff w} \hat{P}^w \right| _{w = \frac{n_\X}{n_\Y + n_\Y}} &= 2p(1-p)\left(
\frac{\frac{n_\X}{n_\Y + n_\Y}}{n_\X} + \frac{\frac{n_\X}{n_\Y + n_\Y}}{n_\Y} - \frac{1}{n_\Y} \right) \\
&= 2p(1-p)\left( \frac{1}{n_\Y + n_\X} + \frac{n_\X}{n_\Y(n_\Y + n_\X)} - \frac{1}{n_\Y} \right) \\
&= 2p(1-p)\left( \frac{n_\Y}{n_\Y(n_\Y + n_\X)} + \frac{n_\X}{n_\Y(n_\Y + n_\X)} - \frac{n_\X + n_\Y}{n_\Y(n_\Y + n_\X)} \right) \\
&= 2p(1-p)(0) \\
&= 0
\end{align*}

Therefore $w = \frac{n_\X}{n_\Y + n_\Y}$ is a stationary point of $\hat{P}^w$. To prove it is a minimum:

\[
\left. \frac{\diff^2}{\diff w^2} \right| _{w = \frac{n_\X}{n_\Y + n_\Y}} = \frac{2p(1 - p)}{n_\X}
\]
Since $p \geq 0$ and $n_\X > 0$, $\frac{\diff^2}{\diff w^2}(\hat{P}^w) > 0$, hence $w = \frac{n_\X}{n_\Y + n_\Y}$ is a minimum for $\hat{P}^w$.
